{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/Natural_Language_Processing_Model/blob/main/BigramLanguageModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT Model**"
      ],
      "metadata": {
        "id": "4pwIx_m7mMQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "17B1sJUpIrVk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt */"
      ],
      "metadata": {
        "id": "g3PwLuYII_OD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd1bc27-31cc-4579-f4a3-20d4d2f08982"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-25 13:54:33--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-11-25 13:54:33 (25.0 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "--2024-11-25 13:54:33--  http://sample_data/\n",
            "Resolving sample_data (sample_data)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘sample_data’\n",
            "FINISHED --2024-11-25 13:54:33--\n",
            "Total wall clock time: 0.3s\n",
            "Downloaded: 1 files, 1.1M in 0.04s (25.0 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"/content/input.txt\", \"r\").read()\n",
        "\n",
        "vocab = sorted(list(set(text)))\n",
        "print(text[:500])\n",
        "\n",
        "encode = lambda x : [vocab.index(i) for i in x]\n",
        "decode = lambda y : [vocab[i] for i in y]\n",
        "\n",
        "txt = \"My name is Ritwik\"\n",
        "enc = encode(txt)\n",
        "dec = decode(enc)\n",
        "\n",
        "print(txt)\n",
        "print(enc)\n",
        "print(dec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPkILevpJHuO",
        "outputId": "7cfedc95-32e7-46ca-8dc3-baa8c173bd8e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n",
            "My name is Ritwik\n",
            "[25, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 30, 47, 58, 61, 47, 49]\n",
            "['M', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'R', 'i', 't', 'w', 'i', 'k']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(len(text) * 0.9)\n",
        "\n",
        "data = encode(text)\n",
        "\n",
        "data_ten = torch.tensor(data, dtype = torch.long)\n",
        "\n",
        "train_data, val_data = data_ten[: split_index], data_ten[split_index:]\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYStKxGCLNfB",
        "outputId": "dfcd010a-d650-4d49-b2a0-6b86b0e6c8ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854\n",
            "111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "block_size = 8\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i : i + block_size] for i in idx])\n",
        "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in idx])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "train_x, train_y = get_batch(\"train\")\n",
        "\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCK48hCER3kM",
        "outputId": "2f407ae3-f665-4d59-a4fa-633e24d0dc8b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8])\n",
            "torch.Size([32, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, embed_size, dropout = 0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(embed_size, head_size, bias = False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias = False)\n",
        "        self.value = nn.Linear(embed_size, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        K = self.key(x)\n",
        "        Q = self.query(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        weight = Q@K.transpose(2, 1) / self.head_size**0.5\n",
        "        weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        weight = F.softmax(weight, dim = 2)\n",
        "        weight = self.dropout(weight)\n",
        "        out = weight@V\n",
        "        return out"
      ],
      "metadata": {
        "id": "W4swvWmMTvZe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*CrM1qbWX8I4_AhVDspEjNA.png)"
      ],
      "metadata": {
        "id": "E0BXE5I6pIIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, head_size, num_heads, embed_size, dropout = 0.3):\n",
        "        super().__init__()\n",
        "        self.m_head = nn.ModuleList([Head(head_size, embed_size) for _ in range(num_heads)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([head(x) for head in self.m_head], dim = -1)\n",
        "        x = self.dropout(self.proj(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "Tz8i8Ud5eTw3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:368/format:webp/1*ilctVE_zmOZv4pKc2SGHcA.png)"
      ],
      "metadata": {
        "id": "YOl-TpTAppi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, dropout = 0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ff(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-qlMPxNNoqy7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = embed_size // num_heads\n",
        "        self.multihead = MultiHeadAttention(head_size, num_heads, embed_size)\n",
        "        self.ff = FeedForward(embed_size)\n",
        "        self.ln1 = nn.LayerNorm(embed_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.multihead(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "SckIWkNSumQV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0N0aHoN6MzSvFloJiSS1Rg.png)"
      ],
      "metadata": {
        "id": "BvdNwcMFobQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_embed = nn.Embedding(block_size, embed_size)\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "        self.block = nn.Sequential(*[Block(embed_size, num_heads) for _ in range(num_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, idx, target = None):\n",
        "        B, T = idx.shape\n",
        "        logits = self.token_embedding(idx)\n",
        "        pos = self.pos_embed(torch.arange(T, device = device))\n",
        "        x = logits + pos\n",
        "        logits = self.block(x)\n",
        "        logits = self.linear(self.layer_norm(logits))\n",
        "\n",
        "        if target == None:\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            target = target.view(B * T)\n",
        "            loss = F.cross_entropy(logits, target)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_token):\n",
        "        idx = idx.to(device)\n",
        "        for _ in range(max_new_token):\n",
        "            crop_idx = idx[:, -block_size:].to(device)\n",
        "            logits, loss = self(crop_idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            prob = F.softmax(logits, dim = -1)\n",
        "            idx_next = torch.multinomial(prob, num_samples = 1).to(device)\n",
        "            idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "75xc16SNL9Xy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size = 65, embed_size= 64, num_heads = 8, num_layers = 4).to(device)"
      ],
      "metadata": {
        "id": "r3Rd6f_KCJj4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "# training the model, cause I won't give up without a fight\n",
        "for epoch in range(5000):\n",
        "\n",
        "    # Printing the Training and Validation Loss\n",
        "    if epoch%1000==0:\n",
        "        model.eval()\n",
        "        Loss= 0.0\n",
        "        Val_Loss = 0.0\n",
        "        for k in range(200):\n",
        "            x, y = get_batch(True)\n",
        "\n",
        "            val_ , val_loss = model(x, y)\n",
        "            x1, y1 = get_batch(False)\n",
        "\n",
        "            _, train_loss = model(x1, y1)\n",
        "            Loss += train_loss.item()\n",
        "            Val_Loss += val_loss.item()\n",
        "        avg_loss = Val_Loss/(k+1)\n",
        "\n",
        "        avg_train_loss = Loss/(k+1)\n",
        "        model.train()\n",
        "\n",
        "        print(\"Epoch: {} \\n The validation loss is:{}    The Loss is:{}\".format(epoch, avg_loss, avg_train_loss))\n",
        "    # Forward\n",
        "    data, target = get_batch(False)\n",
        "    logits, loss = model(data, target)\n",
        "    #Backward\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRvn3Y78zJ_a",
        "outputId": "426cc684-3541-4381-86e6-9d370e48f55f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \n",
            " The validation loss is:4.361562628746032    The Loss is:4.36268013715744\n",
            "Epoch: 1000 \n",
            " The validation loss is:2.171871321797371    The Loss is:2.1896412241458894\n",
            "Epoch: 2000 \n",
            " The validation loss is:2.0455833995342254    The Loss is:2.036001957654953\n",
            "Epoch: 3000 \n",
            " The validation loss is:1.9809299886226654    The Loss is:1.9586844688653946\n",
            "Epoch: 4000 \n",
            " The validation loss is:1.923082172870636    The Loss is:1.9273563402891158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.tensor(encode(\"Before we proceed any further\"), dtype = torch.long).unsqueeze(0)\n",
        "print(idx.shape)\n",
        "print(idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAE9IA-Q2MBA",
        "outputId": "59a1524d-eeb6-4cdd-fdaf-02078c8ddc72"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 29])\n",
            "tensor([[14, 43, 44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1,\n",
            "         39, 52, 63,  1, 44, 59, 56, 58, 46, 43, 56]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\".join(decode(model.generate(torch.zeros([1,1], dtype=torch.long) , max_new_token=2000)[0].tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKMenLvD5XWp",
        "outputId": "3968393e-b7dd-4cd8-c388-9ef42a392c56"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "APTHARINA:\n",
            "Honk\n",
            "PROSPgd thed to ts tid forcess.\n",
            "\n",
            "jedinced widen not in but marr, Shave will stome,\n",
            "Sughth if the your comed.\n",
            "Hir 'is her pre a.\n",
            "\n",
            "PETRUCHIO:\n",
            "\n",
            "Lut thane has tand twendel wha by,\n",
            "To psoke.\n",
            "\n",
            "KATHARIANIOSPe, any. Sor a this ipplife\n",
            "Who herre's fring old the have you. gave.\n",
            "Heware dusek reans in uchas.\n",
            "\n",
            "VINTONDELO:\n",
            "Hars cones prest wich whon soot--ris neved with cosions mive bove reno?\n",
            "\n",
            "PRONIO:\n",
            "Tend! way i cout. comsurtio?\n",
            "\n",
            "FERLUMIRO:\n",
            "Foo eBut sien stel at spry winded they geas bisterg the pingrir-elce, leestleam lico!\n",
            "\n",
            "KARINCANTIO:\n",
            "My hou?\n",
            "\n",
            "MIRANDELLO:\n",
            "Whis wor pentleack to gitens?\n",
            "\n",
            "GREMIO:\n",
            "For it willobkes,\n",
            "Thate\n",
            "Ain.\n",
            "\n",
            "MIRAN:\n",
            "Aven.\n",
            "\n",
            "PEMIRe more?\n",
            "\n",
            "PROSPETRUCHIO:\n",
            "My thit thes my be stirs all a ouger wing to sepce cort.\n",
            "\n",
            "BASTINA:\n",
            "TrANTONIO:\n",
            "\n",
            "HORDANMIO:\n",
            "Alo!\n",
            "But blook that's mald I whine so I ary seeny hy men of my the now, culan\n",
            "unk, whe and,\n",
            "Awaiftair?\n",
            "Why would badzes, I cont leang of her the hake wh krewate.\n",
            "\n",
            "FTENTIO:\n",
            "Nasciocgmementil cast sill?\n",
            "\n",
            "TRANDONIO:\n",
            "Were wor far, a, I sore, and my the crecestremiterting boue?\n",
            "I but conce and; ow.\n",
            "Sir.\n",
            "\n",
            "DRAmdA:\n",
            "Pe iry, her.\n",
            "Ter, an; mor's.\n",
            "\n",
            "GONZALIO:\n",
            "Trays cone:\n",
            "What and and havesel,\n",
            "As he foom.\n",
            "\n",
            "TRANIO:\n",
            "Wheever; oft, deaseg-mon, and in shine\n",
            "A thetere, hew I am thur; if crent; thouser.\n",
            "\n",
            "ARTENTIO,\n",
            "\n",
            "Thou marn, to pay ome moon then.\n",
            "\n",
            "CANCENTIO:\n",
            "If of hever im I is musid oa sa, this me the bey me mone gir tiam,\n",
            "gows veme, Ginme girnio!\n",
            "\n",
            "Mood the no glet the gard, will, as ond his is his pre\n",
            "bass tis but starl, shat bary dhat.\n",
            "\n",
            "PETRUCHIO:\n",
            "Wily the wisons an, orges! bed\n",
            "and\n",
            "Not given:\n",
            "As weldstrues ham day indd compear, I all casle, this b? yfoor whan cathan ming not you wir postsent, learth.\n",
            "\n",
            "NALONINAN:\n",
            "Now ace, w wathe prupt thime muik, hat.\n",
            "\n",
            "GONDELLO:\n",
            "Thbe cret cheap, no'd tane?\n",
            "ne! hine spale, not word, I werd a so nos well genist of stell\n",
            "He, ume crinese;\n",
            "And the or?\n",
            "\n",
            "BOI'll may wplonet emeng on or to the you sleadombles that I wensillo, thouch as I you tay the here welf thangsofe:\n",
            "Yout leavestunnot that prods yat\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8RFauW/Dm5t0OlI7cICiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}